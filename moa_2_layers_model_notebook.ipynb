{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031462,
     "end_time": "2020-11-30T15:09:04.152362",
     "exception": false,
     "start_time": "2020-11-30T15:09:04.120900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mechanisms of Action - Kaggle Competition Solution Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading modules and data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.582235,
     "end_time": "2020-11-30T15:09:06.720031",
     "exception": false,
     "start_time": "2020-11-30T15:09:05.137796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from scipy.special import logit\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.038598,
     "end_time": "2020-11-30T15:09:06.789174",
     "exception": false,
     "start_time": "2020-11-30T15:09:06.750576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ignoring multilabel stratified k fold future warning\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 6.266625,
     "end_time": "2020-11-30T15:09:13.086323",
     "exception": false,
     "start_time": "2020-11-30T15:09:06.819698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try: # kaggle notebook paths\n",
    "    train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "    train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "    train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "    test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "\n",
    "except FileNotFoundError: # local files\n",
    "    train_features = pd.read_csv('train_features.csv')\n",
    "    train_targets_scored = pd.read_csv('train_targets_scored.csv')\n",
    "    train_targets_nonscored = pd.read_csv('train_targets_nonscored.csv')\n",
    "    test_features = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030658,
     "end_time": "2020-11-30T15:09:13.148266",
     "exception": false,
     "start_time": "2020-11-30T15:09:13.117608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.05337,
     "end_time": "2020-11-30T15:09:13.232216",
     "exception": false,
     "start_time": "2020-11-30T15:09:13.178846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "    '''\n",
    "    Combines preprocessing steps into one function\n",
    "    \n",
    "    Parameters:\n",
    "        X (pandas DataFrame): Includes features sig_id, time, dose, and all gene and cell features\n",
    "        \n",
    "    Returns:\n",
    "        X (2D Numpy Array), \n",
    "        cp_type (Pandas Series), \n",
    "        g_features_ids (1D Numpy Array),\n",
    "        c_features_ids (1D Numpy Array),\n",
    "        feature_stats (2D Numpy Array)\n",
    "    '''\n",
    "    # replacing time values with smaller values (24,48,72) -> (1,2,3)\n",
    "    X['cp_time'][X['cp_time']==24] = 0\n",
    "    X['cp_time'][X['cp_time']==48] = 1\n",
    "    X['cp_time'][X['cp_time']==72] = 2\n",
    "\n",
    "    # zero-one encode dose level (high and low)\n",
    "    X['cp_dose'][X['cp_dose']=='D1'] = 1\n",
    "    X['cp_dose'][X['cp_dose']=='D2'] = 0\n",
    "\n",
    "    # remove compound type from training set (ctl_vehicle -> all MOAs == 0)\n",
    "    cp_type = X['cp_type'].copy()\n",
    "    X.drop(columns=['cp_type'], inplace=True)\n",
    "\n",
    "    # remove identifiers from df  \n",
    "    X.drop(columns=['sig_id'], inplace=True)\n",
    "    \n",
    "    # select gene (g) and cell (c) type variables\n",
    "    g_features_names = [item for item in X.columns if item[0]=='g']\n",
    "    g_features_ids = np.arange(2,len(g_features_names)+2)\n",
    "\n",
    "    c_features_names = [item for item in X.columns if item[:2]=='c-']\n",
    "    c_features_ids = np.arange(g_features_ids[-1],len(c_features_names)+g_features_ids[-1])\n",
    "    \n",
    "    X = X.values.astype(float)\n",
    "    \n",
    "    # create statistical variables as additional inputs\n",
    "    feature_stats_list = []\n",
    "    for ids in [g_features_ids, c_features_ids]:\n",
    "        feature_stats_list.append(np.mean(X[:,ids], axis=1).reshape(-1,1))\n",
    "        feature_stats_list.append(np.median(X[:,ids], axis=1).reshape(-1,1))\n",
    "        feature_stats_list.append(np.std(X[:,ids], axis=1).reshape(-1,1))\n",
    "        feature_stats_list.append(np.min(X[:,ids], axis=1).reshape(-1,1))\n",
    "        feature_stats_list.append(np.max(X[:,ids], axis=1).reshape(-1,1))\n",
    "        feature_stats_list.append(kurtosis(X[:,ids], axis=1).reshape(-1,1))\n",
    "        feature_stats_list.append(skew(X[:,ids], axis=1).reshape(-1,1))\n",
    "\n",
    "    feature_stats = np.concatenate(feature_stats_list, axis=1)\n",
    "\n",
    "    return X, cp_type, g_features_ids, c_features_ids, feature_stats \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.044263,
     "end_time": "2020-11-30T15:09:13.307199",
     "exception": false,
     "start_time": "2020-11-30T15:09:13.262936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_pca(X, g_features_ids, c_features_ids, scaling=True, g_ratio=0.95, c_ratio=0.95):\n",
    "    '''\n",
    "    Applies Principle Component Analysis (PCA)\n",
    "    \n",
    "    Parameters:\n",
    "        X (2D Numpy Array): Containing features,\n",
    "        g_features_ids (1D Numpy Array): column ids of gene features,\n",
    "        c_features_ids (1D Numpy Array): column ids of cell features,\n",
    "        scaling (Boolean): Whether to min-max-scale X before applying PCA,\n",
    "        g_ratio (Float): Percantage of variance that should be captured with PCA components of genes range (0,1),\n",
    "        c_ratio (Float): Percantage of variance that should be captured with PCA components of cells range (0,1)\n",
    "        \n",
    "    Returns:\n",
    "        pca_g (sklearn object): object to apply PCA to gene features,\n",
    "        pca_c (sklearn object): object to apply PCA to cell features        \n",
    "    '''\n",
    "    if scaling == True:\n",
    "        scaler = MinMaxScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    pca_g = PCA(len(g_features_ids))\n",
    "    pca_g.fit(X[:,g_features_ids])\n",
    "    components_num_g = np.arange(len(g_features_ids))[np.cumsum(pca_g.explained_variance_ratio_)>=g_ratio][0]\n",
    "\n",
    "    pca_g = PCA(components_num_g)\n",
    "    pca_g.fit(X[:,g_features_ids])\n",
    "\n",
    "    pca_c = PCA(len(c_features_ids))\n",
    "    pca_c.fit(X[:,c_features_ids])\n",
    "    components_num_c = np.arange(len(c_features_ids))[np.cumsum(pca_c.explained_variance_ratio_)>=c_ratio][0]\n",
    "\n",
    "    pca_c = PCA(components_num_c)\n",
    "    pca_c.fit(X[:,c_features_ids])\n",
    "\n",
    "    return pca_g, pca_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031289,
     "end_time": "2020-11-30T15:09:13.369348",
     "exception": false,
     "start_time": "2020-11-30T15:09:13.338059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.038247,
     "end_time": "2020-11-30T15:09:13.438617",
     "exception": false,
     "start_time": "2020-11-30T15:09:13.400370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Set root seed\n",
    "rdm_seed = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.362154,
     "end_time": "2020-11-30T15:09:18.831526",
     "exception": false,
     "start_time": "2020-11-30T15:09:13.469372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# converting chategorical variables and extract cp_type and gene/cell features column ids \n",
    "train_features, train_cp_type, g_features_ids, c_features_ids, train_feature_stats = preprocessing(train_features)\n",
    "test_features, test_cp_type, _, _, test_feature_stats = preprocessing(test_features)\n",
    "\n",
    "# prepare target labels\n",
    "train_targets_scored = train_targets_scored.drop(columns=['sig_id']).values.astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it is not recommended to apply PCA on train and test set combined because this allows for information leakage that would not be possible in a later real world application. However, in this Kaggle Competition it is possible to later apply PCA also on the private test set which will be used for the final performance measurement. Thus, it can potentially improve results due to leaking information of the private test set to the training process. Same applies for the quantile transformation further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.186565,
     "end_time": "2020-11-30T15:09:19.055857",
     "exception": false,
     "start_time": "2020-11-30T15:09:18.869292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine train and test set for pca variables creation\n",
    "X_all = np.concatenate((train_features, test_features))\n",
    "cp_type_all = np.concatenate((train_cp_type, test_cp_type))\n",
    "\n",
    "X_all = X_all[cp_type_all!='ctl_vehicle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 8.982775,
     "end_time": "2020-11-30T15:09:28.070469",
     "exception": false,
     "start_time": "2020-11-30T15:09:19.087694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose gpa ratios \n",
    "g_ratio=0.85\n",
    "c_ratio=0.95\n",
    "\n",
    "# create pca objects for transforming inputs\n",
    "pca_module_g, pca_module_c = apply_pca(X_all, g_features_ids, c_features_ids, \n",
    "                                       g_ratio=g_ratio, c_ratio=c_ratio)\n",
    "\n",
    "# create pca variables for genes and cells\n",
    "pca_g_train_features = pca_module_g.transform(train_features[:,g_features_ids])\n",
    "pca_g_test_features = pca_module_g.transform(test_features[:,g_features_ids])\n",
    "\n",
    "pca_c_train_features = pca_module_c.transform(train_features[:,c_features_ids])\n",
    "pca_c_test_features = pca_module_c.transform(test_features[:,c_features_ids])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.153038,
     "end_time": "2020-11-30T15:09:28.255629",
     "exception": false,
     "start_time": "2020-11-30T15:09:28.102591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adding pca variables to features\n",
    "train_features = np.concatenate((train_features, pca_g_train_features, pca_c_train_features, train_feature_stats), axis=1)\n",
    "test_features = np.concatenate((test_features, pca_g_test_features, pca_c_test_features, test_feature_stats), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Transformation (Gaus Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.206818,
     "end_time": "2020-11-30T15:09:28.494546",
     "exception": false,
     "start_time": "2020-11-30T15:09:28.287728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine train and test set for gaus transformation (works only because applied to private testset later too)\n",
    "X_all = np.concatenate((train_features, test_features))\n",
    "cp_type_all = np.concatenate((train_cp_type, test_cp_type))\n",
    "\n",
    "X_all = X_all[cp_type_all!='ctl_vehicle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 11.594914,
     "end_time": "2020-11-30T15:09:40.124036",
     "exception": false,
     "start_time": "2020-11-30T15:09:28.529122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transforming gene and cell features as well as their pca counterparts to be normally distributed\n",
    "gaus_transformer = QuantileTransformer(n_quantiles=100, \n",
    "                                        output_distribution='normal', \n",
    "                                        random_state=rdm_seed )\n",
    "gaus_transformer.fit(X_all[:,2:])\n",
    "\n",
    "train_features[:,2:] = gaus_transformer.transform(train_features[:,2:])\n",
    "test_features[:,2:] = gaus_transformer.transform(test_features[:,2:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding control vehicles from training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset also includes control trials in which no drugs were used, and thus, no Mechanisms of Action (positive labels) can be found. Hence, it makes sense to exclude these rows from training and set all predictions to 0 for the predictions on the testset at the end.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.205282,
     "end_time": "2020-11-30T15:09:40.361915",
     "exception": false,
     "start_time": "2020-11-30T15:09:40.156633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = train_features[train_cp_type!='ctl_vehicle'] \n",
    "y = train_targets_scored[train_cp_type!='ctl_vehicle'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing device to run model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.39881,
     "end_time": "2020-11-30T15:09:40.913477",
     "exception": false,
     "start_time": "2020-11-30T15:09:40.514667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias term used to initialize final output layer bias allowing for faster conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 4.245412,
     "end_time": "2020-11-30T15:09:45.194419",
     "exception": false,
     "start_time": "2020-11-30T15:09:40.949007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_bias = torch.from_numpy(logit(y.mean(axis=0))).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting testset to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.053869,
     "end_time": "2020-11-30T15:09:45.281641",
     "exception": false,
     "start_time": "2020-11-30T15:09:45.227772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_features = torch.Tensor(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032588,
     "end_time": "2020-11-30T15:09:45.347638",
     "exception": false,
     "start_time": "2020-11-30T15:09:45.315050",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make hyperparameter search (including searching for optimal number of layers) easier, I a class for single layers as well as for the combined model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.048395,
     "end_time": "2020-11-30T15:09:45.428550",
     "exception": false,
     "start_time": "2020-11-30T15:09:45.380155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, input_neurons, output_neurons, dropout_prob, activation, classifier=False):\n",
    "        '''\n",
    "        Creates a single layer that is used inside the HyperParamModel class\n",
    "        \n",
    "        Parameters:\n",
    "            input_neurons (int): Number of inputs,\n",
    "            output_neurons (int): Number of labels, \n",
    "            dropout_prob (float): Probability used for dropout, \n",
    "            activation (string): Name of activation function that should be used (relu, leaky_relu, elu, parametric_relu),\n",
    "            classifier (bolean): Whether layer is output (classifier) layer or not\n",
    "            \n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.activation_dict = nn.ModuleDict([['leaky_relu', nn.LeakyReLU()],\n",
    "                                              ['relu', nn.ReLU()], \n",
    "                                              ['elu', nn.ELU()],\n",
    "                                              ['parametric_relu', nn.PReLU()]])\n",
    "        self.classifier = classifier\n",
    "\n",
    "        if classifier:\n",
    "            self.linear = nn.Linear(input_neurons, output_neurons)\n",
    "            self.linear.bias = nn.parameter.Parameter(output_bias.detach().clone())\n",
    "        else:\n",
    "            self.activation = self.activation_dict[activation]\n",
    "            self.dropout = nn.Dropout(dropout_prob)\n",
    "            \n",
    "            self.batch_norm = nn.BatchNorm1d(output_neurons)\n",
    "            self.linear = nn.Linear(input_neurons, output_neurons)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.classifier:\n",
    "            output = self.linear(input) \n",
    "        else:\n",
    "            output = self.activation(self.dropout(self.batch_norm(self.linear(input))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.046175,
     "end_time": "2020-11-30T15:09:45.507240",
     "exception": false,
     "start_time": "2020-11-30T15:09:45.461065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperParamModel(nn.Module):\n",
    "    '''\n",
    "    Main model class\n",
    "    \n",
    "    Parameters:\n",
    "        input_size (int): Number of inputs, \n",
    "        hidden_size (int): Number of hidden neurons per layer, \n",
    "        output_size (int): Number of output labels, \n",
    "        dropout_prob (float): Probability used for dropout,\n",
    "        num_hidden_layers (int), Number of hidden layers\n",
    "        activation (string): Name of activation function that should be used (relu, leaky_relu, elu, parametric_relu)\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.5, \n",
    "                 num_hidden_layers=2, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.num_layers = num_hidden_layers + 1\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # input layer\n",
    "            if i==0: \n",
    "                self.layers.append(SingleLayer(input_size, hidden_size, dropout_prob, activation))\n",
    "            # output (classifier) layer\n",
    "            elif i==num_hidden_layers: \n",
    "                self.layers.append(SingleLayer(hidden_size, output_size, dropout_prob, activation,\n",
    "                                   classifier=True))\n",
    "            # hidden layers\n",
    "            else: \n",
    "                self.layers.append(SingleLayer(hidden_size, hidden_size, dropout_prob, activation))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.047171,
     "end_time": "2020-11-30T15:09:45.588028",
     "exception": false,
     "start_time": "2020-11-30T15:09:45.540857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Smoothed_BCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, min_clip=1e-6, max_clip=1-1e-6):\n",
    "        super().__init__()\n",
    "        self.min_clip = min_clip\n",
    "        self.max_clip = max_clip\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = torch.clamp(target, self.min_clip, self.max_clip)\n",
    "        return F.binary_cross_entropy_with_logits(input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.044019,
     "end_time": "2020-11-30T15:09:45.665828",
     "exception": false,
     "start_time": "2020-11-30T15:09:45.621809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fct(train_set, batch_size):\n",
    "    model.train()\n",
    "\n",
    "    data = DataLoader(train_set, batch_size=batch_size, shuffle=True)    \n",
    "    \n",
    "    train_loss = 0\n",
    "\n",
    "    for inputs, labels in data:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(inputs)\n",
    "\n",
    "        loss = loss_fct(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss/len(data)\n",
    "\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.044427,
     "end_time": "2020-11-30T15:09:45.743915",
     "exception": false,
     "start_time": "2020-11-30T15:09:45.699488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def val_fct(test_set, batch_size):\n",
    "    model.eval()\n",
    "\n",
    "    data = DataLoader(test_set, batch_size=batch_size, shuffle=False)    \n",
    "    \n",
    "    val_loss = 0\n",
    "\n",
    "    for inputs, labels in data:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)        \n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "            loss = val_loss_fct(output, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss/len(data)\n",
    "\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to predict on Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.043958,
     "end_time": "2020-11-30T15:09:45.821444",
     "exception": false,
     "start_time": "2020-11-30T15:09:45.777486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_fct(test_set, batch_size):\n",
    "    model.eval()\n",
    "\n",
    "    data = DataLoader(test_set, batch_size=batch_size, shuffle=False)    \n",
    "    \n",
    "    predictions = []\n",
    "    for inputs in data:\n",
    "        inputs = inputs.to(device)        \n",
    "        with torch.no_grad():\n",
    "            # in training sigmoid is included into loss function, thus need to add it here \n",
    "            output = torch.sigmoid(model(inputs)) \n",
    "            predictions.append(output)\n",
    "\n",
    "    predictions = torch.cat(predictions).cpu().numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for potential fold specific preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041563,
     "end_time": "2020-11-30T15:09:45.896214",
     "exception": false,
     "start_time": "2020-11-30T15:09:45.854651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fold_preprocessing(X_train, X_val, y_train, y_val):\n",
    "    '''\n",
    "    Preprocessing adjusted to each cross validation fold can be included here.\n",
    "    Only used for Pytorch tensor transformation\n",
    "    '''\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_val = torch.Tensor(X_val)\n",
    "    y_val = torch.Tensor(y_val)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033936,
     "end_time": "2020-11-30T15:09:46.039626",
     "exception": false,
     "start_time": "2020-11-30T15:09:46.005690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.043278,
     "end_time": "2020-11-30T15:09:46.119163",
     "exception": false,
     "start_time": "2020-11-30T15:09:46.075885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maximum number of epochs\n",
    "epochs = 80\n",
    "# Patience used for early stopping\n",
    "patience = 4 \n",
    "# Repetitions of CV scheme\n",
    "repetitions = 3\n",
    "# Number of folds to devide training set into\n",
    "num_folds = 10\n",
    "# Select whether to print information on fewer epochs during training\n",
    "num_epochs_to_print = 2\n",
    "# Whether to shuffle folds in multi label stratified K fold CV \n",
    "shuffle_folds = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.049943,
     "end_time": "2020-11-30T15:09:46.351276",
     "exception": false,
     "start_time": "2020-11-30T15:09:46.301333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyper parameters to search over\n",
    "hyper_params = {'num_hidden_layers': [2],\n",
    "                'batch_size': [128],\n",
    "                'learning_rate': [0.005],\n",
    "                'hidden_size': [256],\n",
    "                'dropout_prob': [0.5],\n",
    "                'activation': ['leaky_relu'],\n",
    "                'label_smoothing':[5e-5],\n",
    "                'weight_decay': [1e-6, 5e-7, 3e-7],\n",
    "                'lr_plateau_factor': [0.1]} \n",
    "\n",
    "# saving best parameters\n",
    "default_params = {'num_hidden_layers': [2],\n",
    "                  'batch_size': [128],\n",
    "                  'learning_rate': [0.005],\n",
    "                  'hidden_size': [256],\n",
    "                  'dropout_prob': [0.5],\n",
    "                  'activation': ['leaky_relu'],\n",
    "                  'label_smoothing':[5e-5],\n",
    "                  'weight_decay': [1e-6],\n",
    "                  'lr_plateau_factor': [0.1]}\n",
    "\n",
    "# select parameter dictionary (hyper_params/default_params)\n",
    "param_grid = ParameterGrid(default_params)\n",
    "# select number of searches | len(param_grid): exhaustive search; int < len(param_grid): random search \n",
    "grid_search_iterations = len(param_grid) \n",
    "\n",
    "random_grid_ids = np.random.choice(len(param_grid), size=grid_search_iterations, replace=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function for validation (no label smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_fct = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1405.456158,
     "end_time": "2020-11-30T15:33:11.910206",
     "exception": false,
     "start_time": "2020-11-30T15:09:46.454048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving best model checkpoints\n",
    "model_checkpoints_all = []\n",
    "model_checkpoints_epochs = []\n",
    "# saving results\n",
    "hyper_search_losses = np.zeros(grid_search_iterations)\n",
    "hyper_search_params = []\n",
    "hyper_search_best_epochs = []\n",
    "hyper_search_results = []\n",
    "\n",
    "# looping over selections of hyper parameters\n",
    "for search_i, grid_id in enumerate(random_grid_ids):\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')    \n",
    "    print(f'\\t{search_i+1}/{grid_search_iterations} hyper parameter combinations to be tested')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "    \n",
    "    random_params = param_grid[grid_id]\n",
    "    hyper_search_params.append(random_params)\n",
    "    \n",
    "    num_hidden_layers = random_params['num_hidden_layers']\n",
    "    batch_size = random_params['batch_size']\n",
    "    learning_rate = random_params['learning_rate']\n",
    "    hidden_size = random_params['hidden_size']\n",
    "    dropout_prob = random_params['dropout_prob']\n",
    "    activation = random_params['activation']\n",
    "    label_smoothing_cuts = random_params['label_smoothing']\n",
    "    weight_decay = random_params['weight_decay']\n",
    "    lr_plateau_factor = random_params['lr_plateau_factor']\n",
    "\n",
    "    loss_fct = Smoothed_BCEWithLogitsLoss(min_clip=label_smoothing_cuts, max_clip=1-label_smoothing_cuts).to(device)\n",
    "    \n",
    "    print('Hyper Parameters:')\n",
    "    print(f'Weight decay: {weight_decay}') \n",
    "    \n",
    "    # losses of all repetitions\n",
    "    all_train_losses = np.array([]).reshape(0,epochs)\n",
    "    all_val_losses = np.array([]).reshape(0,epochs)\n",
    "    \n",
    "    all_best_epochs = []\n",
    "\n",
    "    train_times = []\n",
    "    \n",
    "    test_predictions_all = []\n",
    "    \n",
    "    # looping over random seeds\n",
    "    for rep in range(repetitions):\n",
    "        print('\\t----------------------------------------------------')\n",
    "        print(f'\\t------------------- Repitition {rep+1} -------------------')\n",
    "        print('\\t----------------------------------------------------')\n",
    "        \n",
    "        # different seed per iteration\n",
    "        rep_seed = rdm_seed+rep\n",
    "        torch.manual_seed(rep_seed)\n",
    "        \n",
    "        strat_cv = MultilabelStratifiedKFold(n_splits=num_folds, \n",
    "                                            shuffle=shuffle_folds,\n",
    "                                            random_state=rep_seed)\n",
    "        \n",
    "        # losses per fold\n",
    "        train_losses = np.ones((num_folds,epochs))\n",
    "        val_losses = np.ones((num_folds,epochs))\n",
    "        \n",
    "\n",
    "        start_time = time.time()\n",
    "        # iterating over folds\n",
    "        for i, (train_ids, val_ids) in enumerate(strat_cv.split(X, y)):\n",
    "            print(f'++++++++++++++++++++++++++ Fold Number {i+1:2} ++++++++++++++++++++++++++')\n",
    "            # CV split \n",
    "            X_train = X[train_ids]\n",
    "            y_train = y[train_ids]\n",
    "            X_val = X[val_ids]\n",
    "            y_val = y[val_ids]\n",
    "\n",
    "            # converting arrays into tensors\n",
    "            X_train, X_val, y_train, y_val = fold_preprocessing(X_train, X_val, y_train, y_val)\n",
    "\n",
    "            train_set = TensorDataset(X_train, y_train)\n",
    "            val_set = TensorDataset(X_val, y_val)\n",
    "\n",
    "            # initilize model\n",
    "            input_size = X_train.shape[1]\n",
    "            output_size = y_train.shape[1]\n",
    "\n",
    "            model = HyperParamModel(input_size, hidden_size, output_size, \n",
    "                                    dropout_prob=dropout_prob, num_hidden_layers=num_hidden_layers,\n",
    "                                    activation=activation).to(device)\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "            \n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=lr_plateau_factor, patience=1, \n",
    "                                                                   threshold=1e-6, verbose=True)\n",
    "            # alternitive scheduler\n",
    "            #cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=scheduler_steps, \n",
    "            #                                                              eta_min=min_learning_rate, last_epoch=-1)\n",
    "            \n",
    "\n",
    "            # list with model checkpoints for all epochs per fold\n",
    "            model_checkpoints_fold = []\n",
    "            # lists with losses per epoch\n",
    "            train_loss_list = []\n",
    "            val_loss_list = []\n",
    "\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Model training\n",
    "            for epoch in range(epochs):                \n",
    "                #print(f'Scheduled LR: {cosine_scheduler.get_lr()[0]:.4}')\n",
    "                train_loss = train_fct(train_set, batch_size)\n",
    "                val_loss = val_fct(val_set, batch_size)\n",
    "                \n",
    "                if num_epochs_to_print:\n",
    "                    if epoch < num_epochs_to_print:\n",
    "                        print(f'Epoch {epoch+1:2} | Train Loss: {train_loss:.6}  \\t  Validation Loss: {val_loss:.6}')\n",
    "                else:\n",
    "                    print(f'Epoch {epoch+1:2} | Train Loss: {train_loss:.6}  \\t  Validation Loss: {val_loss:.6}')\n",
    "                \n",
    "                # for early stopping\n",
    "                if epoch == 0:\n",
    "                    pass\n",
    "                elif val_loss > min(val_loss_list):\n",
    "                    patience_counter += 1 \n",
    "                else:\n",
    "                    patience_counter = 0\n",
    "\n",
    "                train_loss_list.append(train_loss)\n",
    "                val_loss_list.append(val_loss)\n",
    "                \n",
    "                # apply early stopping\n",
    "                if patience_counter == patience:\n",
    "                    if num_epochs_to_print:\n",
    "                        print(f'Epoch {epoch+1:2} | Train Loss: {train_loss:.6}  \\t  Validation Loss: {val_loss:.6}')\n",
    "                    print(f'---- Early stopping with best val-loss {np.min(val_loss_list):.6} in epoch {np.argmin(val_loss_list)+1} ----')\n",
    "                    break\n",
    "\n",
    "                scheduler.step(val_loss)\n",
    "                #cosine_scheduler.step()    \n",
    "                #print(f'Scheduled LR: {cosine_scheduler.get_lr()[0]:.4}')\n",
    "\n",
    "                ### save model parameters as checkpoint after each epoch\n",
    "                model_checkpoints_fold.append(model.state_dict().copy())\n",
    "\n",
    "\n",
    "            ### save only best model checkpoint after certain epoch\n",
    "            best_epoch_fold = np.argmin(val_loss_list)\n",
    "            all_best_epochs.append(best_epoch_fold)\n",
    "            model_checkpoints_all.append(model_checkpoints_fold[best_epoch_fold])\n",
    "            model_checkpoints_epochs.append(best_epoch_fold)            \n",
    "            \n",
    "            # save losses of current fold\n",
    "            train_losses[i,:len(train_loss_list)] = np.array(train_loss_list)\n",
    "            val_losses[i,:len(train_loss_list)] = np.array(val_loss_list)\n",
    "\n",
    "        all_train_losses = np.concatenate((all_train_losses, train_losses))\n",
    "        all_val_losses = np.concatenate((all_val_losses, val_losses))\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = int(end_time-start_time)\n",
    "        print('-------------------------')\n",
    "        print(f'Training took {duration} seconds')\n",
    "        print('-------------------------')\n",
    "        train_times.append(duration)\n",
    "    # calculate mean of loss of each fold (out of fold loss)\n",
    "    oof_loss = np.min(all_val_losses, axis=1).mean().round(8)\n",
    "    # calculate std of repetitions\n",
    "    overall_std = np.min(all_val_losses, axis=1).std().round(8)\n",
    "    over_repetition_means_std = np.min(all_val_losses, axis=1).reshape(-1, repetitions).mean(axis=0).std().round(8)\n",
    "    # add oof loss to array\n",
    "    hyper_search_losses[search_i] = oof_loss\n",
    "    # save best performing epoch of each fold\n",
    "    hyper_search_best_epochs.append(all_best_epochs)\n",
    "    # add oof loss to parameters for results dataframe    \n",
    "    random_params['oof_loss'] = oof_loss\n",
    "#    random_params['snap_oof_loss'] = snap_oof_loss\n",
    "    random_params['overall_std'] = overall_std\n",
    "    random_params['over_repetition_means_std'] = over_repetition_means_std\n",
    "\n",
    "    random_params['time per seed'] = np.mean(train_times).round(0)\n",
    "    hyper_search_results.append(random_params)\n",
    "    \n",
    "    test_predictions_all = np.array(test_predictions_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.016345,
     "end_time": "2020-11-30T15:33:13.866909",
     "exception": false,
     "start_time": "2020-11-30T15:33:12.850564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyper_search_results = pd.DataFrame(hyper_search_results)\n",
    "\n",
    "hyper_search_results.sort_values('oof_loss', inplace=True)\n",
    "\n",
    "hyper_search_results.to_csv('hyper_search_results.csv', index=False)\n",
    "\n",
    "hyper_search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.686238,
     "end_time": "2020-11-30T15:33:15.333598",
     "exception": false,
     "start_time": "2020-11-30T15:33:14.647360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.81505,
     "end_time": "2020-11-30T15:33:16.834730",
     "exception": false,
     "start_time": "2020-11-30T15:33:16.019680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "except FileNotFoundError: \n",
    "    sample_submission = pd.read_csv('sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with each model saved during CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 4.477684,
     "end_time": "2020-11-30T15:33:23.393311",
     "exception": false,
     "start_time": "2020-11-30T15:33:18.915627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions_checkpoints = np.zeros((len(model_checkpoints_all), test_features.shape[0], y.shape[1]))\n",
    "\n",
    "for i, checkpoint in enumerate(model_checkpoints_all):\n",
    "    model.load_state_dict(checkpoint)\n",
    "    test_predictions = predict_fct(test_features, batch_size)\n",
    "\n",
    "    test_predictions_checkpoints[i,:,:] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.791369,
     "end_time": "2020-11-30T15:33:24.873615",
     "exception": false,
     "start_time": "2020-11-30T15:33:24.082246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = test_predictions_checkpoints.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clipping pridictions because of high penalty for highly confident wrong predictions due to log loss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.717068,
     "end_time": "2020-11-30T15:33:26.307969",
     "exception": false,
     "start_time": "2020-11-30T15:33:25.590901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = np.clip(test_predictions, 0.001, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change all predictions to 0 for control vehicles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.69736,
     "end_time": "2020-11-30T15:33:27.690444",
     "exception": false,
     "start_time": "2020-11-30T15:33:26.993084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions[test_cp_type=='ctl_vehicle'] = np.zeros(test_predictions.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill and save submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.997115,
     "end_time": "2020-11-30T15:33:29.434005",
     "exception": false,
     "start_time": "2020-11-30T15:33:28.436890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission.iloc[:,1:] = test_predictions\n",
    "submission = sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.174255,
     "end_time": "2020-11-30T15:33:32.298914",
     "exception": false,
     "start_time": "2020-11-30T15:33:30.124659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "papermill": {
   "duration": 1475.140049,
   "end_time": "2020-11-30T15:33:35.148816",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-30T15:09:00.008767",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
